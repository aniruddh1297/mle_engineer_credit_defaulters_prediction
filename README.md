# ğŸ§  Credit Card Default Prediction using Azure Machine Learning

## ğŸ“Œ Overview

This project builds a **production-ready ML pipeline** to predict whether a credit card client will default on their payment next month. The solution uses **Azure Machine Learning**, implements **MLOps best practices**, and includes support for **deployment, CI/CD, explainability, and business impact metrics**.

## ğŸ’¼ Business Problem

Defaulting credit card customers significantly impact financial institutions. Accurately identifying high-risk clients enables the bank to take preventive action. This model predicts defaults using customer behavior and credit profile.

## ğŸ—ï¸ Architecture

- Azure ML SDK v2 + MLFlow
- Multi-component SDK pipeline: preprocess â†’ train â†’ evaluate
- Cross-environment setup: dev, test, prod workspaces
- Model promotion strategy from test â†’ prod
- Automated deployment via Azure Online Endpoints
- CI/CD with GitHub Actions (includes conditional triggers for dev/test/prod)
- Environment and component registration handled programmatically

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .github/workflows/            # CI/CD workflows
â”œâ”€â”€ data/                         # Raw dataset
â”œâ”€â”€ components/                   # Azure ML components
â”‚   â”œâ”€â”€ preprocess_component.py
â”‚   â”œâ”€â”€ train_component.py
â”‚   â””â”€â”€ evaluate_component.py
â”œâ”€â”€ pipelines/                    # Run & register scripts
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â””â”€â”€ promote_model.py
â”œâ”€â”€ serve/                        # Deployment artifacts
â”‚   â”œâ”€â”€ score.py
â”‚   â”œâ”€â”€ inference_config.yaml
â”‚   â”œâ”€â”€ deploy_endpoint.py
â”‚   â””â”€â”€ sample_request.json
â”œâ”€â”€ environments/                 # Conda environment YAMLs
â”œâ”€â”€ config/                       # Workspace config files
â”œâ”€â”€ register_scripts/            # Environment/component promotion
â”‚   â”œâ”€â”€ register_env.py
â”‚   â”œâ”€â”€ register_component.py
â”‚   â””â”€â”€ promote_model.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§ª ML Pipeline Components

- ğŸ”¹ **Preprocessing**: Scales data, handles missing values
- ğŸ”¹ **Training**: Trains XGBoost, Logistic Regression, Random Forest; logs best model via MLflow
- ğŸ”¹ **Evaluation**:
  - Classification metrics (F1, ROC-AUC, Precision/Recall)
  - Cost-sensitive threshold optimization
  - SHAP explainability
  - Confusion matrix & curves logged to MLflow

## ğŸš€ Deployment

The best model is deployed via Azure ML Online Endpoint:

- `score.py`: Inference script using MLflow
- `deploy_endpoint.py`: Script to deploy endpoint
- `inference_config.yaml`: Configuration for endpoint creation
- `sample_request.json`: Test payload

You can invoke the endpoint using:
```bash
curl -X POST <ENDPOINT_URL> -H "Authorization: Bearer <TOKEN>" -d @sample_request.json
```

## ğŸ” Model Promotion Strategy

- Models are registered in the **test workspace** and evaluated.
- If they pass performance thresholds, they are:
  - Promoted via `promote_model.py` to the **prod workspace**
  - Automatically deployed via CI/CD if the commit message includes `deploy to prod`

> ğŸ”’ NOTE: The model is currently not tagged as `validated`. This is a planned future enhancement.

## âœ… CI/CD with GitHub Actions

CI/CD workflow automates:
- Azure ML environment & component registration
- Triggering the training pipeline
- Promoting the best model to prod
- Deploying the model to a managed online endpoint

Customizable triggers:
- Commit message with `pipeline-start`: runs pipeline even on `dev`
- Commit message with `prod-run`: forces full pipeline in `main` (prod)
- Otherwise, promotion from test to prod `main`
- As for Test env , the pipeline will always run keeping in mind reproducibility and verification in mind

CI/CD Workflow:
```yaml
# Azure ML Multi-Env Pipeline CI
# (see .github/workflows/azureml-pipeline-ci.yml)

on:
  push:
    branches:
      - release/dev
      - release/test
      - main
    paths:
      - "component_code/**.yaml"
      - "component_code/**.py"
      - "pipeline/run_pipeline.py"
      - "register_scripts/**.py"
      - "promote_model.py"
      - "config/**.json"
      - "requirements.txt"
      - ".github/workflows/azureml-pipeline-ci.yml"
```

## ğŸ“ˆ Model Performance

| Metric       | Value   |
|--------------|---------|
| Accuracy     | 77.1%   |
| F1 Score     | 55.1%   |
| ROC-AUC      | 76.5%   |
| Precision    | 48.6%   |
| Recall       | 56.8%   |
| Optimal Threshold | 0.69 |
| Estimated Cost | ~764K |

All metrics are logged to MLflow.

## ğŸ§  Explainability

SHAP values are computed and logged during evaluation. Beeswarm plots highlight most influential features in predictions. Saved as `shap_beeswarm.png` and logged to MLflow.

## ğŸ”’ Cost & Security

- Cost-sensitive thresholding built into evaluation.
- Small instance types used (e.g., `Standard_E2s_v3`).
- Credentials managed securely via Azure CLI & GitHub Secrets.

## ğŸ“ How to Run Locally

1. Clone the repo
2. Set up environment:
   ```bash
   pip install -r requirements.txt
   az login
   ```
3. Run:
   ```bash
   python register_scripts/register_env.py --env dev
   python register_scripts/register_component.py --env dev
   python pipelines/run_pipeline.py --env dev
   ```

To deploy:
```bash
cd serve
python deploy_endpoint.py
```

## ğŸ§ª Endpoint Testing

Use `sample_request.json` and test with:
```bash
curl -X POST <URL> -H "Authorization: Bearer <TOKEN>" -d @sample_request.json
```

## ğŸ™Œ Acknowledgments

- Azure ML SDK v2
- MLOps best practices
- MLflow for experiment tracking
- scikit-learn, XGBoost

#test